{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering with side information\n",
    "** *\n",
    "This IPython notebook illustrates the usage of the [cmfrec](https://github.com/david-cortes/cmfrec) Python package for collective matrix factorization using the [MovieLens-1M data](https://grouplens.org/datasets/movielens/1m/), consisting of ratings from users about movies + user demographic information, plus the [movie tag genome](https://grouplens.org/datasets/movielens/latest/).\n",
    "\n",
    "Collective matrix factorization is a technique for collaborative filtering with additional information about the users and items, based on low-rank joint factorization of different matrices with shared factors â€“ for more details see the paper [_Singh, A. P., & Gordon, G. J. (2008, August). Relational learning via collective matrix factorization. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 650-658). ACM._](http://ra.adm.cs.cmu.edu/anon/usr/ftp/ml2008/CMU-ML-08-109.pdf).\n",
    "\n",
    "** Small note: if the TOC here is not clickable or the math symbols don't show properly, try visualizing this same notebook from nbviewer following [this link](http://nbviewer.jupyter.org/github/david-cortes/cmfrec/blob/master/example/cmfrec_movielens_sideinfo.ipynb). **\n",
    "** *\n",
    "## Sections\n",
    "\n",
    "\n",
    "[1. Model description](#p1)\n",
    "\n",
    "[2. Loading the data](#p2)\n",
    "* [2.1 Ratings data](#p21)\n",
    "* [2.2 Creating a train and test split](#p22)\n",
    "* [2.3 Processing item tags](#p23)\n",
    "* [2.4 Processing user demographic info](#p24)\n",
    "\n",
    "[3. Basic model - only movie ratings](#p3)\n",
    "* [3.1 Fitting the model](#p31)\n",
    "* [3.2 Evaluating results](#p32)\n",
    "\n",
    "[4. Model with user side information](#p4)\n",
    "* [4.1 Original version](#p41)\n",
    "* [4.2 Offsets model](#p42)\n",
    "\n",
    "[5. Model with item side information](#p5)\n",
    "* [5.1 Original version](#p51)\n",
    "* [5.2 Offsets model](#p52)\n",
    "\n",
    "[6. Full model](#p6)\n",
    "* [6.1 Original version](#p61)\n",
    "* [6.2 Offsets model](#p62)\n",
    "\n",
    "[7. Examining some recomendations](#p7)\n",
    "** *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p1\"></a>\n",
    "# 1. Model description\n",
    "\n",
    "The colective matrix mactorization model is an extension of the typical low-rank matrix factorization model to incorporate user and/or item side information. In its most basic form, low-rank matrix factorization tries to find an approximation of a matrix $X$ given by two lower-rank matrices $A$ and $B$, which in recommender systems would represent, respectively, a matrix of latent factors for users and items, which are determined by minimizing the squared differences between their product and $X$, i.e.:\n",
    "\n",
    "$$ argmin_{A, B} \\lVert X - AB^T \\lVert$$\n",
    "\n",
    "This basic formula can be improved by adding regularization on the $A$ and $B$ matrices, as well as by centering the matrix $X$ by substracting its global mean for each entry, adding user and item biases, and considering only the non-missing entries, i.e.:\n",
    "\n",
    "$$ argmin_{A, B, U_b, I_b} \\lVert (X - \\mu - U_b - I_b - AB^T)I_{x} \\lVert^2 + \\lambda (\\lVert A\\lVert^2 + \\lVert B \\lVert^2 + \\lVert U_b \\lVert^2 + \\lVert I_b \\lVert^2) $$\n",
    "\n",
    "Where:\n",
    "* $X$ is the ratings matrix (entry in row $\\{ i,j\\}$ contains the rating given by user $i$ to item $j$).\n",
    "* $A$ and $B$ are lower-dimensional matrices (model parameters).\n",
    "* $U_b$ is a column matrix of user biases, containing at each row a constant for its respective user.\n",
    "* $I_b$ is a row matrix of item biases, containing at each column a constant for its respective item.\n",
    "* $mu$ is the mean of the entries in $X$.\n",
    "* $I_x$ is an indicator matrix with entries in row $\\{i,j\\}$ equal to one when that same entry is present in the matrix $X$, and equal to zero when the corresponding entry is missing in $X$.\n",
    "* $\\lambda$ is a regularization parameter.\n",
    "\n",
    "In collective matrix factorization, this model is further extended by also factorizing matrices of user and/or item side information (e.g. movie tags, user demographic info in tabular format, etc.)., e.g.:\n",
    "\n",
    "$$ argmin_{A, B, C, D, U_b, I_b} \\lVert (X - \\mu - U_b - I_b - AB^T)I_{x} \\lVert^2 + \\lVert U - AC^T \\lVert^2 + \\lVert I - BD^T \\lVert^2 + \\lambda (\\lVert A\\lVert^2 + \\lVert B \\lVert^2 + \\lVert C \\lVert^2 + \\lVert D \\lVert^2  + \\lVert U_b \\lVert^2 + \\lVert I_b \\lVert^2) $$\n",
    "\n",
    "Where, in addition to the previous model:\n",
    "* $U$ is the user side information matrix.\n",
    "* $I$ is the item side information matrix.\n",
    "* $C$ is a matrix of latent factors for user attributes (model parameters).\n",
    "* $D$ is a matrix of latent factors for item attributes (model parameters).\n",
    "\n",
    "(Other variations such as different weights for each factorization, different regularization for each paramter, and most notably, applying a sigmoid function on factorized values for binary variables, among others, are also possible to fit with this package).\n",
    "\n",
    "Intuitively, latent factors that also do a good job at explainin user/item attributes should generalize better to ratings data than latent factors that don't, even though it might adversely affect training error in the factorization of interest (the $X$ matrix).\n",
    "\n",
    "Alternatively, the package can also use a different formulation, in which the user and/or item attributes can be though of as the base of the factorization, with additional latent matrices acting as offsets for each user and item (deviations from its expected ratings according to the side information), e.g.:\n",
    "\n",
    "$$ argmin_{A, B, C, D, U_b, I_b} \\lVert (X - \\mu - U_b - I_b - (UC + A)(ID + B)\\:)I_{x} \\lVert^2 + \\lambda (\\lVert A\\lVert^2 + \\lVert B \\lVert^2 + \\lVert C \\lVert^2 + \\lVert D \\lVert^2 + \\lVert U_b \\lVert^2 + \\lVert I_b \\lVert^2) $$\n",
    "\n",
    "Both of these models allow for making recommendations based only on user/item side information without ratings, in the first case by either training the model with extra users/items, or by computing the corresponding rows/columns of $A$ and $B$ by minimizing *only* the factorization of $U$ and $I$ (for a new user/item, there won't be any new entries in $C$ or $D$), which can be done in closed form; and in the second case, by setting the corresponding rows/columns of $A$ and $B$ to zero. As the ratings are centered, the expected value of both $U_b$ and $I_b$ are zero, which aids in cold-start recommendations.\n",
    "\n",
    "** *\n",
    "<a id=\"p2\"></a>\n",
    "# 2. Loading the data\n",
    "\n",
    "This example notebook uses the MovieLens 1M dataset, with movie tags taken from the latest MovieLens release, and user demographic information linked to the user information provided in the dataset, by taking a [publicly available table](http://federalgovernmentzipcodes.us/) mapping zip codes to states, [another one](http://www.fonz.net/blog/archives/2008/04/06/csv-of-states-and-state-abbreviations/) mapping state names to their abbreviations, and finally classifying the states into regions according to [usual definitions](https://www.infoplease.com/us/states/sizing-states).\n",
    "\n",
    "Unfortunately, later (bigger) release of the MovieLens dataset no longer include include user demographic information.\n",
    "\n",
    "<a id=\"p21\"></a>\n",
    "## 2.1 Ratings data\n",
    "\n",
    "The ratings come in the form of a table with columns UserId, ItemId, Rating, and Timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  ItemId  Rating\n",
       "0       1    1193       5\n",
       "1       1     661       3\n",
       "2       1     914       3\n",
       "3       1    3408       4\n",
       "4       1    2355       5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, time, re\n",
    "from datetime import datetime\n",
    "from cmfrec import CMF\n",
    "\n",
    "ratings = pd.read_table('~/movielens/ml-1m/ratings.dat', sep='::',\n",
    "                        engine='python', names=['UserId','ItemId','Rating','Timestamp'])\n",
    "del ratings['Timestamp']\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p22\"></a>\n",
    "## 2.2 Creating a train and test split\n",
    "\n",
    "Usually, a good way to test recommender models is by temporal splits (splitting the data at some temporal cutoff point between train and test), but in this case, it's more desirable to make a distinction between warm start (predicting ratings strictly for users and items that were in the training data), and different forms of cold-start, i.e.: completely new users and items, new users with known items, and vice versa, which I'll try do here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478105, 3)\n",
      "(84358, 3)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "user_ids = ratings.UserId.drop_duplicates().values\n",
    "item_ids = ratings.ItemId.drop_duplicates().values\n",
    "users_train = set(np.random.choice(user_ids, size=int(user_ids.shape[0] * .75), replace=False))\n",
    "items_train = set(np.random.choice(item_ids, size=int(item_ids.shape[0] * .75), replace=False))\n",
    "train = ratings.loc[ratings.UserId.isin(users_train) & ratings.ItemId.isin(items_train)].reset_index(drop=True)\n",
    "\n",
    "np.random.seed(1)\n",
    "train_ix = train.sample(frac=.85).index\n",
    "test_ix = np.setdiff1d(train.index.values, train_ix)\n",
    "test_warm_start = train.loc[test_ix].reset_index(drop=True)\n",
    "train = train.loc[train_ix].reset_index(drop=True)\n",
    "users_train = set(train.UserId)\n",
    "items_train = set(train.ItemId)\n",
    "test_warm_start = test_warm_start.loc[test_warm_start.UserId.isin(users_train) &\n",
    "                                      test_warm_start.ItemId.isin(items_train)].reset_index(drop=True)\n",
    "\n",
    "test_cold_start = ratings.loc[~ratings.UserId.isin(users_train) & ~ratings.ItemId.isin(items_train)].reset_index(drop=True)\n",
    "test_new_users = ratings.loc[(~ratings.UserId.isin(users_train)) & (ratings.ItemId.isin(items_train))].reset_index(drop=True)\n",
    "test_new_items = ratings.loc[(ratings.UserId.isin(users_train)) & (~ratings.ItemId.isin(items_train))].reset_index(drop=True)\n",
    "test_new_items_set = set(test_new_items.ItemId)\n",
    "users_coldstart = set(test_cold_start.UserId)\n",
    "items_coldstart = set(test_cold_start.ItemId)\n",
    "\n",
    "print(train.shape)\n",
    "print(test_warm_start.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4530\n",
      "2753\n"
     ]
    }
   ],
   "source": [
    "print(len(users_train))\n",
    "print(len(items_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p23\"></a>\n",
    "## 2.3 Processing item tags\n",
    "\n",
    "Item tags were taken from the latest MovieLens release, and joined to the dataset by movie title, which is not a perfect match but does a reasonable job. They are unfortunately not available for all the movies for which there are ratings.\n",
    "\n",
    "For the second model, as the dimensionality of the tags is quite high, I'll also take a smaller transformation consisting of the first 50 principal components of these tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemId</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>tag3</th>\n",
       "      <th>tag4</th>\n",
       "      <th>tag5</th>\n",
       "      <th>tag6</th>\n",
       "      <th>tag7</th>\n",
       "      <th>tag8</th>\n",
       "      <th>tag9</th>\n",
       "      <th>...</th>\n",
       "      <th>tag1119</th>\n",
       "      <th>tag1120</th>\n",
       "      <th>tag1121</th>\n",
       "      <th>tag1122</th>\n",
       "      <th>tag1123</th>\n",
       "      <th>tag1124</th>\n",
       "      <th>tag1125</th>\n",
       "      <th>tag1126</th>\n",
       "      <th>tag1127</th>\n",
       "      <th>tag1128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02475</td>\n",
       "      <td>0.02475</td>\n",
       "      <td>0.04900</td>\n",
       "      <td>0.07750</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.23875</td>\n",
       "      <td>0.06575</td>\n",
       "      <td>0.28575</td>\n",
       "      <td>0.25400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.02050</td>\n",
       "      <td>0.04300</td>\n",
       "      <td>0.03375</td>\n",
       "      <td>0.12375</td>\n",
       "      <td>0.04150</td>\n",
       "      <td>0.02125</td>\n",
       "      <td>0.03600</td>\n",
       "      <td>0.10425</td>\n",
       "      <td>0.02750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.03750</td>\n",
       "      <td>0.04100</td>\n",
       "      <td>0.03675</td>\n",
       "      <td>0.04750</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.05950</td>\n",
       "      <td>0.05125</td>\n",
       "      <td>0.09600</td>\n",
       "      <td>0.08875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03425</td>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.02325</td>\n",
       "      <td>0.13525</td>\n",
       "      <td>0.02450</td>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.01325</td>\n",
       "      <td>0.08550</td>\n",
       "      <td>0.01925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.02750</td>\n",
       "      <td>0.03250</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.02275</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.03050</td>\n",
       "      <td>0.01700</td>\n",
       "      <td>0.06500</td>\n",
       "      <td>0.02625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03075</td>\n",
       "      <td>0.02025</td>\n",
       "      <td>0.01525</td>\n",
       "      <td>0.02075</td>\n",
       "      <td>0.21150</td>\n",
       "      <td>0.02450</td>\n",
       "      <td>0.01925</td>\n",
       "      <td>0.00975</td>\n",
       "      <td>0.08125</td>\n",
       "      <td>0.01675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.03175</td>\n",
       "      <td>0.03600</td>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.03950</td>\n",
       "      <td>0.01375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.01125</td>\n",
       "      <td>0.01125</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.01175</td>\n",
       "      <td>0.00650</td>\n",
       "      <td>0.08350</td>\n",
       "      <td>0.01725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.99975</td>\n",
       "      <td>0.99975</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>0.03400</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.04100</td>\n",
       "      <td>0.04575</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.06550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49425</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>0.02950</td>\n",
       "      <td>0.16275</td>\n",
       "      <td>0.04600</td>\n",
       "      <td>0.02075</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.07250</td>\n",
       "      <td>0.01875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemId     tag1     tag2     tag3     tag4    tag5     tag6     tag7  \\\n",
       "0       1  0.02475  0.02475  0.04900  0.07750  0.1245  0.23875  0.06575   \n",
       "1       2  0.03750  0.04100  0.03675  0.04750  0.1000  0.05950  0.05125   \n",
       "2       8  0.02750  0.03250  0.04250  0.02275  0.0545  0.03050  0.01700   \n",
       "3       9  0.03175  0.03600  0.01750  0.01650  0.0330  0.01500  0.01350   \n",
       "4      10  0.99975  0.99975  0.01900  0.03400  0.0605  0.04100  0.04575   \n",
       "\n",
       "      tag8     tag9   ...     tag1119  tag1120  tag1121  tag1122  tag1123  \\\n",
       "0  0.28575  0.25400   ...     0.03125  0.02050  0.04300  0.03375  0.12375   \n",
       "1  0.09600  0.08875   ...     0.03425  0.01825  0.01650  0.02325  0.13525   \n",
       "2  0.06500  0.02625   ...     0.03075  0.02025  0.01525  0.02075  0.21150   \n",
       "3  0.03950  0.01375   ...     0.01825  0.01125  0.01125  0.01475  0.15250   \n",
       "4  0.12000  0.06550   ...     0.49425  0.02250  0.02100  0.02950  0.16275   \n",
       "\n",
       "   tag1124  tag1125  tag1126  tag1127  tag1128  \n",
       "0  0.04150  0.02125  0.03600  0.10425  0.02750  \n",
       "1  0.02450  0.01825  0.01325  0.08550  0.01925  \n",
       "2  0.02450  0.01925  0.00975  0.08125  0.01675  \n",
       "3  0.02175  0.01175  0.00650  0.08350  0.01725  \n",
       "4  0.04600  0.02075  0.01575  0.07250  0.01875  \n",
       "\n",
       "[5 rows x 1129 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles = pd.read_table('~/movielens/ml-1m/movies.dat',\n",
    "                             sep='::', engine='python', header=None)\n",
    "movie_titles.columns = ['ItemId', 'title', 'genres']\n",
    "movie_titles = movie_titles[['ItemId', 'title']]\n",
    "\n",
    "# will save the movie titles for later\n",
    "movie_id_to_title = {i.ItemId:i.title for i in movie_titles.itertuples()}\n",
    "\n",
    "movies = pd.read_csv('~/movielens/ml-latest/movies.csv')\n",
    "movies = movies[['movieId', 'title']]\n",
    "movies = pd.merge(movies, movie_titles)\n",
    "movies = movies[['movieId', 'ItemId']]\n",
    "\n",
    "tags = pd.read_csv('~/movielens/ml-latest/genome-scores.csv')\n",
    "tags_wide = tags.pivot(index='movieId', columns='tagId', values='relevance')\n",
    "tags_wide.columns=[\"tag\"+str(i) for i in tags_wide.columns.values]\n",
    "\n",
    "item_side_info = pd.merge(movies, tags_wide, how='inner', left_on='movieId', right_index=True)\n",
    "del item_side_info['movieId']\n",
    "items_w_sideinfo = set(item_side_info.ItemId)\n",
    "test_new_items = test_new_items.loc[test_new_items.ItemId.isin(items_w_sideinfo)].reset_index(drop=True)\n",
    "item_sideinfo_train = item_side_info.loc[item_side_info.ItemId.isin(items_train)].reset_index(drop=True)\n",
    "item_sideinfo_testnew = item_side_info.loc[item_side_info.ItemId.isin(test_new_items_set)].reset_index(drop=True)\n",
    "test_cold_start = test_cold_start.loc[test_cold_start.ItemId.isin(items_w_sideinfo)].reset_index(drop=True)\n",
    "item_sideinfo_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc0</th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>pc3</th>\n",
       "      <th>pc4</th>\n",
       "      <th>pc5</th>\n",
       "      <th>pc6</th>\n",
       "      <th>pc7</th>\n",
       "      <th>pc8</th>\n",
       "      <th>pc9</th>\n",
       "      <th>...</th>\n",
       "      <th>pc41</th>\n",
       "      <th>pc42</th>\n",
       "      <th>pc43</th>\n",
       "      <th>pc44</th>\n",
       "      <th>pc45</th>\n",
       "      <th>pc46</th>\n",
       "      <th>pc47</th>\n",
       "      <th>pc48</th>\n",
       "      <th>pc49</th>\n",
       "      <th>ItemId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.174361</td>\n",
       "      <td>2.454127</td>\n",
       "      <td>2.032595</td>\n",
       "      <td>-1.171341</td>\n",
       "      <td>0.298269</td>\n",
       "      <td>1.355678</td>\n",
       "      <td>-0.692807</td>\n",
       "      <td>-1.044511</td>\n",
       "      <td>2.065727</td>\n",
       "      <td>-0.531681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055836</td>\n",
       "      <td>-0.167791</td>\n",
       "      <td>-0.311193</td>\n",
       "      <td>0.319949</td>\n",
       "      <td>0.133470</td>\n",
       "      <td>0.029714</td>\n",
       "      <td>-0.302162</td>\n",
       "      <td>0.147117</td>\n",
       "      <td>-0.369435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.340086</td>\n",
       "      <td>1.930136</td>\n",
       "      <td>1.029146</td>\n",
       "      <td>-0.354897</td>\n",
       "      <td>-0.436367</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>-0.554990</td>\n",
       "      <td>-0.605132</td>\n",
       "      <td>1.111225</td>\n",
       "      <td>-0.534643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>0.178682</td>\n",
       "      <td>-0.467821</td>\n",
       "      <td>-0.037618</td>\n",
       "      <td>-0.283306</td>\n",
       "      <td>0.275545</td>\n",
       "      <td>-0.072865</td>\n",
       "      <td>0.452367</td>\n",
       "      <td>0.014473</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.536161</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>0.944357</td>\n",
       "      <td>0.387365</td>\n",
       "      <td>-0.196264</td>\n",
       "      <td>-0.093866</td>\n",
       "      <td>-0.670976</td>\n",
       "      <td>-0.110382</td>\n",
       "      <td>0.694794</td>\n",
       "      <td>-0.322744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116378</td>\n",
       "      <td>0.228412</td>\n",
       "      <td>-0.248827</td>\n",
       "      <td>-0.186690</td>\n",
       "      <td>-0.427460</td>\n",
       "      <td>0.028590</td>\n",
       "      <td>-0.077187</td>\n",
       "      <td>0.132514</td>\n",
       "      <td>-0.102053</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.044958</td>\n",
       "      <td>0.482059</td>\n",
       "      <td>-0.208248</td>\n",
       "      <td>0.825030</td>\n",
       "      <td>0.029346</td>\n",
       "      <td>-0.384561</td>\n",
       "      <td>0.184310</td>\n",
       "      <td>0.901687</td>\n",
       "      <td>0.433354</td>\n",
       "      <td>0.701625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060544</td>\n",
       "      <td>-0.021556</td>\n",
       "      <td>-0.083973</td>\n",
       "      <td>-0.121497</td>\n",
       "      <td>0.065433</td>\n",
       "      <td>-0.317927</td>\n",
       "      <td>0.307823</td>\n",
       "      <td>-0.281140</td>\n",
       "      <td>-0.166976</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.786717</td>\n",
       "      <td>1.928858</td>\n",
       "      <td>0.032624</td>\n",
       "      <td>0.888157</td>\n",
       "      <td>0.171164</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>0.596246</td>\n",
       "      <td>0.824269</td>\n",
       "      <td>0.295776</td>\n",
       "      <td>0.813716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177851</td>\n",
       "      <td>-0.259741</td>\n",
       "      <td>-0.095441</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>-0.233801</td>\n",
       "      <td>0.206002</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.309817</td>\n",
       "      <td>0.092431</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pc0       pc1       pc2       pc3       pc4       pc5       pc6  \\\n",
       "0  1.174361  2.454127  2.032595 -1.171341  0.298269  1.355678 -0.692807   \n",
       "1 -1.340086  1.930136  1.029146 -0.354897 -0.436367  0.379800 -0.554990   \n",
       "2 -1.536161 -0.006325  0.944357  0.387365 -0.196264 -0.093866 -0.670976   \n",
       "3 -2.044958  0.482059 -0.208248  0.825030  0.029346 -0.384561  0.184310   \n",
       "4 -0.786717  1.928858  0.032624  0.888157  0.171164 -0.035373  0.596246   \n",
       "\n",
       "        pc7       pc8       pc9   ...        pc41      pc42      pc43  \\\n",
       "0 -1.044511  2.065727 -0.531681   ...    0.055836 -0.167791 -0.311193   \n",
       "1 -0.605132  1.111225 -0.534643   ...    0.095954  0.178682 -0.467821   \n",
       "2 -0.110382  0.694794 -0.322744   ...   -0.116378  0.228412 -0.248827   \n",
       "3  0.901687  0.433354  0.701625   ...   -0.060544 -0.021556 -0.083973   \n",
       "4  0.824269  0.295776  0.813716   ...   -0.177851 -0.259741 -0.095441   \n",
       "\n",
       "       pc44      pc45      pc46      pc47      pc48      pc49  ItemId  \n",
       "0  0.319949  0.133470  0.029714 -0.302162  0.147117 -0.369435       1  \n",
       "1 -0.037618 -0.283306  0.275545 -0.072865  0.452367  0.014473       2  \n",
       "2 -0.186690 -0.427460  0.028590 -0.077187  0.132514 -0.102053       8  \n",
       "3 -0.121497  0.065433 -0.317927  0.307823 -0.281140 -0.166976       9  \n",
       "4 -0.276813 -0.233801  0.206002  0.016063  0.309817  0.092431      10  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_obj = PCA(n_components = 50)\n",
    "item_sideinfo_reduced = item_side_info.copy()\n",
    "del item_sideinfo_reduced['ItemId']\n",
    "pca_obj.fit(item_sideinfo_reduced)\n",
    "item_sideinfo_pca = pca_obj.transform(item_sideinfo_reduced)\n",
    "item_sideinfo_pca = pd.DataFrame(item_sideinfo_pca)\n",
    "item_sideinfo_pca.columns = [\"pc\"+str(i) for i in range(item_sideinfo_pca.shape[1])]\n",
    "item_sideinfo_pca['ItemId'] = item_side_info.ItemId.values.copy()\n",
    "\n",
    "item_sideinfo_pca_train = item_sideinfo_pca.loc[item_sideinfo_pca.ItemId.isin(items_train)].reset_index(drop=True)\n",
    "item_sideinfo_pca_testnew = item_sideinfo_pca.loc[item_sideinfo_pca.ItemId.isin(test_new_items_set)].reset_index(drop=True)\n",
    "item_sideinfo_pca_coldstart = item_sideinfo_pca.loc[item_sideinfo_pca.ItemId.isin(items_coldstart)].reset_index(drop=True)\n",
    "item_sideinfo_pca_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171405\n",
      "759\n"
     ]
    }
   ],
   "source": [
    "print(test_new_items.shape[0])\n",
    "print(test_new_items.ItemId.drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p24\"></a>\n",
    "## 2.4 Processing user demographic info\n",
    "\n",
    "The extra data is exaplained at the beginning. Joining all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david_cortes_rivera/anaconda3/envs/py3d/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>Gender_F</th>\n",
       "      <th>Gender_M</th>\n",
       "      <th>Age_1</th>\n",
       "      <th>Age_18</th>\n",
       "      <th>Age_25</th>\n",
       "      <th>Age_35</th>\n",
       "      <th>Age_45</th>\n",
       "      <th>Age_50</th>\n",
       "      <th>Age_56</th>\n",
       "      <th>...</th>\n",
       "      <th>Occupation_8</th>\n",
       "      <th>Occupation_9</th>\n",
       "      <th>Region_Middle Atlantic</th>\n",
       "      <th>Region_Midwest</th>\n",
       "      <th>Region_New England</th>\n",
       "      <th>Region_South</th>\n",
       "      <th>Region_Southwest</th>\n",
       "      <th>Region_UnknownOrNonUS</th>\n",
       "      <th>Region_UsOther</th>\n",
       "      <th>Region_West</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  Gender_F  Gender_M  Age_1  Age_18  Age_25  Age_35  Age_45  Age_50  \\\n",
       "0       2         0         1      0       0       0       0       0       0   \n",
       "1       4         0         1      0       0       0       0       1       0   \n",
       "2       5         0         1      0       0       1       0       0       0   \n",
       "3       6         1         0      0       0       0       0       0       1   \n",
       "4       7         0         1      0       0       0       1       0       0   \n",
       "\n",
       "   Age_56     ...       Occupation_8  Occupation_9  Region_Middle Atlantic  \\\n",
       "0       1     ...                  0             0                       0   \n",
       "1       0     ...                  0             0                       0   \n",
       "2       0     ...                  0             0                       0   \n",
       "3       0     ...                  0             1                       0   \n",
       "4       0     ...                  0             0                       0   \n",
       "\n",
       "   Region_Midwest  Region_New England  Region_South  Region_Southwest  \\\n",
       "0               0                   0             1                 0   \n",
       "1               0                   1             0                 0   \n",
       "2               1                   0             0                 0   \n",
       "3               1                   0             0                 0   \n",
       "4               0                   1             0                 0   \n",
       "\n",
       "   Region_UnknownOrNonUS  Region_UsOther  Region_West  \n",
       "0                      0               0            0  \n",
       "1                      0               0            0  \n",
       "2                      0               0            0  \n",
       "3                      0               0            0  \n",
       "4                      0               0            0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipcode_abbs = pd.read_csv(\"~/movielens/states.csv\")\n",
    "zipcode_abbs_dct = {z.State:z.Abbreviation for z in zipcode_abbs.itertuples()}\n",
    "us_regs_table = [\n",
    "    ('New England', 'Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont'),\n",
    "    ('Middle Atlantic', 'Delaware, Maryland, New Jersey, New York, Pennsylvania'),\n",
    "    ('South', 'Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, Missouri, North Carolina, South Carolina, Tennessee, Virginia, West Virginia'),\n",
    "    ('Midwest', 'Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin'),\n",
    "    ('Southwest', 'Arizona, New Mexico, Oklahoma, Texas'),\n",
    "    ('West', 'Alaska, California, Colorado, Hawaii, Idaho, Montana, Nevada, Oregon, Utah, Washington, Wyoming')\n",
    "    ]\n",
    "us_regs_table = [(x[0], [i.strip() for i in x[1].split(\",\")]) for x in us_regs_table]\n",
    "us_regs_dct = dict()\n",
    "for r in us_regs_table:\n",
    "    for s in r[1]:\n",
    "        us_regs_dct[zipcode_abbs_dct[s]] = r[0]\n",
    "        \n",
    "zipcode_info = pd.read_csv(\"~/movielens/free-zipcode-database.csv\")\n",
    "zipcode_info = zipcode_info.groupby('Zipcode').first().reset_index()\n",
    "zipcode_info['State'].loc[zipcode_info.Country != \"US\"] = 'UnknownOrNonUS'\n",
    "zipcode_info['Region'] = zipcode_info['State'].copy()\n",
    "zipcode_info['Region'].loc[zipcode_info.Country == \"US\"] = \\\n",
    "        zipcode_info.Region\\\n",
    "        .loc[zipcode_info.Country == \"US\"]\\\n",
    "        .map(lambda x: us_regs_dct[x] if x in us_regs_dct else 'UsOther')\n",
    "zipcode_info = zipcode_info[['Zipcode', 'Region']]\n",
    "\n",
    "\n",
    "users = pd.read_table('~/movielens/ml-1m/users.dat',\n",
    "                      sep='::', names=[\"UserId\", \"Gender\", \"Age\", \"Occupation\", \"Zipcode\"], engine='python')\n",
    "users[\"Zipcode\"] = users.Zipcode.map(lambda x: np.int(re.sub(\"-.*\",\"\",x)))\n",
    "users = pd.merge(users,zipcode_info,on='Zipcode',how='left')\n",
    "users['Region'] = users.Region.fillna('UnknownOrNonUS')\n",
    "\n",
    "users['Occupation'] = users.Occupation.map(lambda x: str(x))\n",
    "users['Age'] = users.Age.map(lambda x: str(x))\n",
    "user_side_info = pd.get_dummies(users[['UserId', 'Gender', 'Age', 'Occupation', 'Region']])\n",
    "users_w_sideinfo = set(user_side_info.UserId)\n",
    "test_new_users = test_new_users.loc[test_new_users.ItemId.isin(users_w_sideinfo)].reset_index(drop=True)\n",
    "user_sideinfo_train = user_side_info.loc[user_side_info.UserId.isin(users_train)].reset_index(drop=True)\n",
    "test_cold_start = test_cold_start.loc[test_cold_start.UserId.isin(users_w_sideinfo)].reset_index(drop=True)\n",
    "user_sideinfo_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187456\n",
      "1510\n"
     ]
    }
   ],
   "source": [
    "print(test_new_users.shape[0])\n",
    "print(test_new_users.UserId.drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57290\n",
      "1509\n",
      "737\n"
     ]
    }
   ],
   "source": [
    "print(test_cold_start.shape[0])\n",
    "print(test_cold_start.UserId.unique().shape[0])\n",
    "print(test_cold_start.ItemId.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *\n",
    "<a id=\"p3\"></a>\n",
    "# 3. Basic model - only movie ratings\n",
    "\n",
    "Non-collective factorization model - including user and item biases + regularization:\n",
    "\n",
    "<a id=\"p31\"></a>\n",
    "## 3.1 Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "  Objective function value: 0.797731\n",
      "  Number of iterations: 212\n",
      "  Number of functions evaluations: 224\n",
      "CPU times: user 6min 24s, sys: 43.6 s, total: 7min 8s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from copy import deepcopy\n",
    "from cmfrec import CMF\n",
    "\n",
    "model_no_side_info = CMF(k=40, reg_param=1e-4, random_seed=1)\n",
    "model_no_side_info.fit(deepcopy(train))\n",
    "test_warm_start['Predicted'] = model_no_side_info.predict(test_warm_start.UserId, test_warm_start.ItemId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p32\"></a>\n",
    "## 3.2 Evaluating results\n",
    "\n",
    "For this model and the ones that will follow, I will evaluate the recommendations by computing:\n",
    "* Root mean squared error (RMSE), i.e. sum( sqrt( (real - predicted)^2 ) ) - which can be though of the average star-rating error for each predicted rating. This is the most typical measure but has some drawbacks as it doesn't tend to be a good measure when ranking and can be substantially improved without changing the relative order of predictions.\n",
    "* Taking the average rating of the top-10 recommended movies for each user.\n",
    "\n",
    "There are other more appropriate evaluation criteria, but these are easy to understand and provide reasonable insights on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (no side info, warm start):  0.8683360112941739\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE (no side info, warm start): \", np.sqrt(np.mean( (test_warm_start.Predicted - test_warm_start.Rating)**2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge movie rating: 3.695626272928932\n",
      "Average rating for top-10 rated by each user: 4.236997084548105\n",
      "Average rating for bottom-10 rated by each user: 3.0766763848396503\n",
      "Average rating for top-10 recommendations of best-rated movies: 3.9557725947521867\n",
      "----------------------\n",
      "Average rating for top-10 recommendations from this model: 4.001166180758018\n",
      "Average rating for bottom-10 (non-)recommendations from this model: 3.333177842565598\n"
     ]
    }
   ],
   "source": [
    "avg_ratings = train.groupby('ItemId')['Rating'].mean().to_frame().rename(columns={\"Rating\" : \"AvgRating\"})\n",
    "test_ = pd.merge(test_warm_start, avg_ratings, left_on='ItemId', right_index=True, how='left')\n",
    "\n",
    "print('Averge movie rating:', test_.groupby('UserId')['Rating'].mean().mean())\n",
    "print('Average rating for top-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=True).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for top-10 recommendations of best-rated movies:', test_.sort_values(['UserId','AvgRating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('----------------------')\n",
    "print('Average rating for top-10 recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 (non-)recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=True).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *\n",
    "<a id=\"p4\"></a>\n",
    "# 4. Model with user side information\n",
    "\n",
    "Now I'll add only the user information (without the movie tags). These are exclusively binary columns (only 0/1 values), so I'll apply a sigmoid function on the factorized values to be between zero and one.\n",
    "\n",
    "<a id=\"p41\"></a>\n",
    "## 4.1 Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 8.225498\n",
      "  Number of iterations: 208\n",
      "  Number of functions evaluations: 229\n",
      "CPU times: user 7min 5s, sys: 44.2 s, total: 7min 49s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_user_info = CMF(k=40, w_main=10.0, w_user=1.0, reg_param=1e-3, random_seed=1)\n",
    "model_user_info.fit(deepcopy(train),\n",
    "                    user_info=deepcopy(user_sideinfo_train),\n",
    "                    cols_bin_user=[cl for cl in user_side_info.columns if cl!='UserId'])\n",
    "test_warm_start['Predicted'] = model_user_info.predict(test_warm_start.UserId, test_warm_start.ItemId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, new users can be incorporated into the model without refitting it entirely from scratch (a very slow procedure on larger datasets), but this is quite slow to do with many users. The following code would do it, but for time reasons it was not executed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for u in list(test_new_users.UserId.unique()):\n",
    "#     user_vec = user_side_info.loc[user_side_info.UserId == u]\n",
    "#     del user_vec['UserId']\n",
    "#     user_vec = user_vec.values.reshape((1, -1))\n",
    "#     model_user_info.add_user(new_id = u, attributes = user_vec)\n",
    "# test_new_users['Predicted'] = model_user_info.predict(test_new_users.UserId, test_new_users.ItemId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side information from users which have no ratings can still be incorporated, and predictions can also be made for these users despite not having any ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 8.226118\n",
      "  Number of iterations: 238\n",
      "  Number of functions evaluations: 283\n",
      "CPU times: user 9min 22s, sys: 55.1 s, total: 10min 17s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_user_info_all = CMF(k=40, w_main=10.0, w_user=1.0, reg_param=1e-3, random_seed=1)\n",
    "model_user_info_all.fit(deepcopy(train),\n",
    "                        user_info=deepcopy(user_side_info),\n",
    "                        cols_bin_user=[cl for cl in user_side_info.columns if cl!='UserId'])\n",
    "test_warm_start['PredictedAll'] = model_user_info_all.predict(test_warm_start.UserId, test_warm_start.ItemId)\n",
    "test_new_users['PredictedAll'] = model_user_info_all.predict(test_new_users.UserId, test_new_users.ItemId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics are the same as before plus simple correlation coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (user side info, warm start):  0.8682231678918979\n",
      "RMSE (user side info, warm start, extra users):  0.8680965530731076\n",
      "RMSE (user side info, users trained without ratings):  0.969946692788752\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE (user side info, warm start): \", np.sqrt(np.mean( (test_warm_start.Predicted - test_warm_start.Rating)**2) ))\n",
    "print(\"RMSE (user side info, warm start, extra users): \", np.sqrt(np.mean( (test_warm_start.PredictedAll - test_warm_start.Rating)**2) ))\n",
    "# print(\"RMSE (user side info, new users, added afterwards): \", np.sqrt(np.mean( (test_new_users.Predicted - test_new_users.Rating)**2) ))\n",
    "print(\"RMSE (user side info, users trained without ratings): \", np.sqrt(np.mean( (test_new_users.PredictedAll - test_new_users.Rating)**2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rho (user side info, warm start):  0.6417988677672211\n",
      "Rho (user side info, warm start, extra users):  0.6419802709408267\n",
      "Rho (user side info, users trained without ratings):  0.47893410436098627\n"
     ]
    }
   ],
   "source": [
    "print(\"Rho (user side info, warm start): \", np.corrcoef(test_warm_start.Predicted, test_warm_start.Rating)[0][1])\n",
    "print(\"Rho (user side info, warm start, extra users): \", np.corrcoef(test_warm_start.PredictedAll, test_warm_start.Rating)[0][1])\n",
    "# print(\"RMSE (user side info, new users, added afterwards): \", np.corrcoef(test_new_users.Predicted, test_new_users.Rating)[0][1])\n",
    "print(\"Rho (user side info, users trained without ratings): \", np.corrcoef(test_new_users.PredictedAll, test_new_users.Rating)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge movie rating: 3.695626272928932\n",
      "Average rating for top-10 rated by each user: 4.236997084548105\n",
      "Average rating for bottom-10 rated by each user: 3.0766763848396503\n",
      "Average rating for top-10 recommendations of best-rated movies: 3.9557725947521867\n",
      "----------------------\n",
      "Average rating for top-10 recommendations from this model: 4.000495626822158\n",
      "Average rating for bottom-10 (non-)recommendations from this model: 3.3337026239067056\n"
     ]
    }
   ],
   "source": [
    "avg_ratings = train.groupby('ItemId')['Rating'].mean().to_frame().rename(columns={\"Rating\" : \"AvgRating\"})\n",
    "test_ = pd.merge(test_warm_start, avg_ratings, left_on='ItemId', right_index=True, how='left')\n",
    "\n",
    "print('Averge movie rating:', test_.groupby('UserId')['Rating'].mean().mean())\n",
    "print('Average rating for top-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=True).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for top-10 recommendations of best-rated movies:', test_.sort_values(['UserId','AvgRating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('----------------------')\n",
    "print('Average rating for top-10 recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 (non-)recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=True).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for top-10 recommendations (per user) from this model per configuration\n",
      "warm start:  4.000495626822158\n",
      "warm start, extra users:  4.002244897959184\n",
      "users trained without ratings:  4.317880794701987\n"
     ]
    }
   ],
   "source": [
    "print('Average rating for top-10 recommendations (per user) from this model per configuration')\n",
    "print('warm start: ', test_warm_start.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('warm start, extra users: ', test_warm_start.sort_values(['UserId','PredictedAll'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "# print('new users, added afterwards: ', test_new_users.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('users trained without ratings: ', test_new_users.sort_values(['UserId','PredictedAll'], ascending=False).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a slight improvement over not using user demographic information, and as shown, it gets a slight advantage by incorporating side information from more users than there are ratings from. It seems to perform surprisingly well on users trained without ratings too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p42\"></a>\n",
    "## 4.2 Offsets model\n",
    "\n",
    "Alternative formulation as explained in [section 1](#p1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 0.707400\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1047\n",
      "CPU times: user 37min 30s, sys: 4min 15s, total: 41min 45s\n",
      "Wall time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_user_info2 = CMF(k=40, reg_param=1e-4, offsets_model=True, random_seed=1)\n",
    "model_user_info2.fit(deepcopy(train),\n",
    "                     user_info = deepcopy(user_sideinfo_train))\n",
    "test_warm_start['Predicted'] = model_user_info2.predict(test_warm_start.UserId, test_warm_start.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 36 ms, total: 2.48 s\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for u in list(test_new_users.UserId.unique()):\n",
    "    user_vec = deepcopy(user_side_info.loc[user_side_info.UserId == u])\n",
    "    del user_vec['UserId']\n",
    "    user_vec = user_vec.values.reshape((1, -1))\n",
    "    model_user_info2.add_user(new_id = u, attributes = user_vec)\n",
    "test_new_users['Predicted'] = model_user_info2.predict(test_new_users.UserId, test_new_users.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (user side info, warm start):  0.8987353773892749\n",
      "RMSE (user side info, new users, added afterwards):  0.9800443420437788\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE (user side info, warm start): \", np.sqrt(np.mean( (test_warm_start.Predicted - test_warm_start.Rating)**2) ))\n",
    "print(\"RMSE (user side info, new users, added afterwards): \", np.sqrt(np.mean( (test_new_users.Predicted - test_new_users.Rating)**2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rho (user side info, warm start):  0.6054951333024773\n",
      "Rho (user side info, new users, added afterwards):  0.4611727674305657\n"
     ]
    }
   ],
   "source": [
    "print(\"Rho (user side info, warm start): \", np.corrcoef(test_warm_start.Predicted, test_warm_start.Rating)[0][1])\n",
    "print(\"Rho (user side info, new users, added afterwards): \", np.corrcoef(test_new_users.Predicted, test_new_users.Rating)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge movie rating: 3.695626272928932\n",
      "Average rating for top-10 rated by each user: 4.236997084548105\n",
      "Average rating for bottom-10 rated by each user: 3.0766763848396503\n",
      "Average rating for top-10 recommendations of best-rated movies: 3.9557725947521867\n",
      "----------------------\n",
      "Average rating for top-10 recommendations from this model: 3.976209912536443\n",
      "Average rating for bottom-10 (non-)recommendations from this model: 3.3616909620991255\n"
     ]
    }
   ],
   "source": [
    "avg_ratings = train.groupby('ItemId')['Rating'].mean().to_frame().rename(columns={\"Rating\" : \"AvgRating\"})\n",
    "test_ = pd.merge(test_warm_start, avg_ratings, left_on='ItemId', right_index=True, how='left')\n",
    "\n",
    "print('Averge movie rating:', test_.groupby('UserId')['Rating'].mean().mean())\n",
    "print('Average rating for top-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=True).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for top-10 recommendations of best-rated movies:', test_.sort_values(['UserId','AvgRating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('----------------------')\n",
    "print('Average rating for top-10 recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 (non-)recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=True).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for top-10 recommendations (per user) from this model per configuration\n",
      "warm start:  3.976209912536443\n",
      "new users, added afterwards:  4.2864238410596025\n"
     ]
    }
   ],
   "source": [
    "print('Average rating for top-10 recommendations (per user) from this model per configuration')\n",
    "print('warm start: ', test_warm_start.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('new users, added afterwards: ', test_new_users.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this formulation doesn't seem to perform as well as the previous one in either case, but it is much faster to add new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p5\"></a>\n",
    "# 5. Model with item side information\n",
    "\n",
    "Like before, now fitting the collective model incorporating movie tags, but not user information\n",
    "\n",
    "<a id=\"p51\"></a>\n",
    "## 5.1 Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 7.981097\n",
      "  Number of iterations: 229\n",
      "  Number of functions evaluations: 252\n",
      "CPU times: user 11min 36s, sys: 1min 6s, total: 12min 43s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_item_info = CMF(k=35, k_main=15, k_item=10, reg_param=1e-3, w_main=10.0, w_item=0.5, random_seed=1)\n",
    "model_item_info.fit(deepcopy(train),\n",
    "                    item_info = deepcopy(item_sideinfo_train))\n",
    "test_warm_start['Predicted'] = model_item_info.predict(test_warm_start.UserId, test_warm_start.ItemId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, it's possible to add new items to the model without having to refit it entirely from scratch, but this is very slow to do with many items and was not run here for time reasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in test_new_items.ItemId.unique():\n",
    "#     item_vec = item_side_info.loc[item_side_info.ItemId == i]\n",
    "#     del user_vec['ItemId']\n",
    "#     item_vec = item_vec.values.reshape((1, -1))\n",
    "#     model_user_info.add_item(new_id = i, attributes = user_vec)\n",
    "# test_new_items['Predicted'] = model_item_info.predict(test_new_items.UserId, test_new_items.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 7.981080\n",
      "  Number of iterations: 228\n",
      "  Number of functions evaluations: 254\n",
      "CPU times: user 12min 46s, sys: 1min 9s, total: 13min 56s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_item_info_all = CMF(k=35, k_main=15, k_item=10, reg_param=1e-3, w_main=10.0, w_item=0.5, random_seed=1)\n",
    "model_item_info_all.fit(deepcopy(train), item_info = deepcopy(item_side_info))\n",
    "test_warm_start['PredictedAll'] = model_item_info_all.predict(test_warm_start.UserId, test_warm_start.ItemId)\n",
    "test_new_items['PredictedAll'] = model_item_info_all.predict(test_new_items.UserId, test_new_items.ItemId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, I will also try a version that puts more emphasis in correct factorization of the side information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 4.644912\n",
      "  Number of iterations: 84\n",
      "  Number of functions evaluations: 115\n",
      "CPU times: user 5min 38s, sys: 31.7 s, total: 6min 10s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_item_info_diffweight = CMF(k=50, k_main=0, k_item=0, reg_param=1e-3, w_main=5.0, w_item=5.0, random_seed=1)\n",
    "model_item_info_diffweight.fit(deepcopy(train), item_info = deepcopy(item_side_info))\n",
    "test_warm_start['PredictedAll2'] = model_item_info_diffweight.predict(test_warm_start.UserId, test_warm_start.ItemId)\n",
    "test_new_items['PredictedAll2'] = model_item_info_diffweight.predict(test_new_items.UserId, test_new_items.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (item side info, warm start):  0.8680824066936402\n",
      "RMSE (item side info, warm start, extra items):  0.868025522651484\n",
      "RMSE (item side info, warm start, extra items, diff. weighting):  0.9060629960504393\n",
      "RMSE (item side info, items trained without ratings):  1.0466672746197183\n",
      "RMSE (item side info, items trained without ratings, diff. weighting):  1.0459891847280063\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE (item side info, warm start): \", np.sqrt(np.mean( (test_warm_start.Predicted - test_warm_start.Rating)**2) ))\n",
    "print(\"RMSE (item side info, warm start, extra items): \", np.sqrt(np.mean( (test_warm_start.PredictedAll - test_warm_start.Rating)**2) ))\n",
    "print(\"RMSE (item side info, warm start, extra items, diff. weighting): \", np.sqrt(np.mean( (test_warm_start.PredictedAll2 - test_warm_start.Rating)**2) ))\n",
    "# print(\"RMSE (item side info, new items, added afterwards): \", np.sqrt(np.mean( (test_new_items.Predicted - test_new_items.Rating)**2) ))\n",
    "print(\"RMSE (item side info, items trained without ratings): \", np.sqrt(np.mean( (test_new_items.PredictedAll - test_new_items.Rating)**2) ))\n",
    "print(\"RMSE (item side info, items trained without ratings, diff. weighting): \", np.sqrt(np.mean( (test_new_items.PredictedAll2 - test_new_items.Rating)**2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rho (item side info, warm start):  0.642168442991823\n",
      "Rho (item side info, warm start, extra items):  0.6422664472770928\n",
      "Rho (item side info, warm start, extra items, diff. weighting):  0.6041898919052913\n",
      "Rho (item side info, items trained without ratings):  0.3559967656248473\n",
      "Rho (item side info, items trained without ratings, diff. weighting):  0.35785580363289865\n"
     ]
    }
   ],
   "source": [
    "print(\"Rho (item side info, warm start): \", np.corrcoef(test_warm_start.Predicted, test_warm_start.Rating)[0][1])\n",
    "print(\"Rho (item side info, warm start, extra items): \", np.corrcoef(test_warm_start.PredictedAll, test_warm_start.Rating)[0][1])\n",
    "print(\"Rho (item side info, warm start, extra items, diff. weighting): \", np.corrcoef(test_warm_start.PredictedAll2, test_warm_start.Rating)[0][1])\n",
    "# print(\"Rho (item side info, new items, added afterwards): \", np.corrcoef(test_new_items.Predicted, test_new_items.Rating)[0][1])\n",
    "print(\"Rho (item side info, items trained without ratings): \", np.corrcoef(test_new_items.PredictedAll, test_new_items.Rating)[0][1])\n",
    "print(\"Rho (item side info, items trained without ratings, diff. weighting): \", np.corrcoef(test_new_items.PredictedAll2, test_new_items.Rating)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge movie rating: 3.695626272928932\n",
      "Average rating for top-10 rated by each user: 4.236997084548105\n",
      "Average rating for bottom-10 rated by each user: 3.0766763848396503\n",
      "Average rating for top-10 recommendations of best-rated movies: 3.9557725947521867\n",
      "----------------------\n",
      "Average rating for top-10 recommendations from this model: 4.002944606413994\n",
      "Average rating for bottom-10 (non-)recommendations from this model: 3.333469387755102\n"
     ]
    }
   ],
   "source": [
    "avg_ratings = train.groupby('ItemId')['Rating'].mean().to_frame().rename(columns={\"Rating\" : \"AvgRating\"})\n",
    "test_ = pd.merge(test_warm_start, avg_ratings, left_on='ItemId', right_index=True, how='left')\n",
    "\n",
    "print('Averge movie rating:', test_.groupby('UserId')['Rating'].mean().mean())\n",
    "print('Average rating for top-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=True).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for top-10 recommendations of best-rated movies:', test_.sort_values(['UserId','AvgRating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('----------------------')\n",
    "print('Average rating for top-10 recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 (non-)recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=True).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for top-10 recommendations (per user) from this model per configuration\n",
      "warm start:  4.002944606413994\n",
      "warm start, extra items:  4.00265306122449\n",
      "items trained without ratings:  3.8518708354689903\n",
      "items trained without ratings, diff. weighting:  3.9085450684630594\n"
     ]
    }
   ],
   "source": [
    "print('Average rating for top-10 recommendations (per user) from this model per configuration')\n",
    "print('warm start: ', test_warm_start.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('warm start, extra items: ', test_warm_start.sort_values(['UserId','PredictedAll'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "# print('new items, added afterwards: ', test_new_items.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('items trained without ratings: ', test_new_items.sort_values(['UserId','PredictedAll'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('items trained without ratings, diff. weighting: ', test_new_items.sort_values(['UserId','PredictedAll2'], ascending=False).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement is comparable to that form adding user side information in the warm-start case, but for new items, it seems the recommendations are not as good as for new users. Putting a heaver weight in the movie tags factorization didn't seem to make it perform better in cold-start according to ranking metrics, but it did bring a slight improvement in terms of RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p52\"></a>\n",
    "## 5.2 Offsets model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "  Objective function value: 0.525121\n",
      "  Number of iterations: 472\n",
      "  Number of functions evaluations: 489\n",
      "CPU times: user 24min 21s, sys: 2min 27s, total: 26min 49s\n",
      "Wall time: 5min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_item_info2 = CMF(k=50, reg_param=1e-4, offsets_model=True, random_seed=1)\n",
    "model_item_info2.fit(deepcopy(train), item_info = deepcopy(item_sideinfo_pca_train))\n",
    "test_warm_start['Predicted'] = model_item_info2.predict(test_warm_start.UserId, test_warm_start.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 s, sys: 40 ms, total: 1.54 s\n",
      "Wall time: 941 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in list(test_new_items.ItemId.unique()):\n",
    "    item_vec = deepcopy(item_sideinfo_pca.loc[item_sideinfo_pca.ItemId == i])\n",
    "    del item_vec['ItemId']\n",
    "    item_vec = item_vec.values.reshape((1, -1))\n",
    "    model_item_info2.add_item(new_id = i, attributes = item_vec)\n",
    "test_new_items['Predicted'] = model_item_info2.predict(test_new_items.UserId, test_new_items.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (item side info, warm start):  0.9268640553404283\n",
      "RMSE (item side info, new items, added afterwards):  0.9567803858196762\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE (item side info, warm start): \", np.sqrt(np.mean( (test_warm_start.Predicted - test_warm_start.Rating)**2) ))\n",
    "print(\"RMSE (item side info, new items, added afterwards): \", np.sqrt(np.mean( (test_new_items.Predicted - test_new_items.Rating)**2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rho (item side info, warm start):  0.5950739127670134\n",
      "Rho (item side info, new items, added afterwards):  0.5636488072210143\n"
     ]
    }
   ],
   "source": [
    "print(\"Rho (item side info, warm start): \", np.corrcoef(test_warm_start.Predicted, test_warm_start.Rating)[0][1])\n",
    "print(\"Rho (item side info, new items, added afterwards): \", np.corrcoef(test_new_items.Predicted, test_new_items.Rating)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge movie rating: 3.695626272928932\n",
      "Average rating for top-10 rated by each user: 4.236997084548105\n",
      "Average rating for bottom-10 rated by each user: 3.0766763848396503\n",
      "Average rating for top-10 recommendations of best-rated movies: 3.9557725947521867\n",
      "----------------------\n",
      "Average rating for top-10 recommendations from this model: 3.9714285714285715\n",
      "Average rating for bottom-10 (non-)recommendations from this model: 3.352069970845481\n"
     ]
    }
   ],
   "source": [
    "avg_ratings = train.groupby('ItemId')['Rating'].mean().to_frame().rename(columns={\"Rating\" : \"AvgRating\"})\n",
    "test_ = pd.merge(test_warm_start, avg_ratings, left_on='ItemId', right_index=True, how='left')\n",
    "\n",
    "print('Averge movie rating:', test_.groupby('UserId')['Rating'].mean().mean())\n",
    "print('Average rating for top-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 rated by each user:', test_.sort_values(['UserId','Rating'], ascending=True).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for top-10 recommendations of best-rated movies:', test_.sort_values(['UserId','AvgRating'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('----------------------')\n",
    "print('Average rating for top-10 recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('Average rating for bottom-10 (non-)recommendations from this model:', test_.sort_values(['UserId','Predicted'], ascending=True).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for top-10 recommendations (per user) from this model per configuration\n",
      "warm start:  3.9714285714285715\n",
      "new items, added afterwards:  4.036586854116326\n"
     ]
    }
   ],
   "source": [
    "print('Average rating for top-10 recommendations (per user) from this model per configuration')\n",
    "print('warm start: ', test_warm_start.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('new items, added afterwards: ', test_new_items.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This alternative formulation doesn't perform as well for warm-start recommendations (in this regards, it seems even worse than when not incorporating it), but it performs significantly better for cold-start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p6\"></a>\n",
    "# 6. Full model\n",
    "\n",
    "Now a model incorporating both user and item side information, fit to extra users and items without any ratings in the training set. Note that the hyperparameters of this model are a lot harder to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "  Objective function value: 0.797726\n",
      "  Number of iterations: 427\n",
      "  Number of functions evaluations: 446\n",
      "CPU times: user 25min 16s, sys: 2min 8s, total: 27min 25s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_user_item_info = CMF(k=40, k_main=10, k_user=5, k_item=15,\n",
    "                           w_main=1.0, w_user=2.0, w_item=0.05,\n",
    "                           reg_param=5e-5, random_seed=1)\n",
    "model_user_item_info.fit(deepcopy(train),\n",
    "                         user_info=deepcopy(user_side_info),\n",
    "                         item_info=deepcopy(item_side_info),\n",
    "                         cols_bin_user=[cl for cl in user_side_info.columns if cl!='UserId'])\n",
    "test_warm_start['PredictedAll'] = model_user_item_info.predict(test_warm_start.UserId, test_warm_start.ItemId)\n",
    "test_cold_start['PredictedAll'] = model_user_item_info.predict(test_cold_start.UserId, test_cold_start.ItemId)\n",
    "test_new_users['PredictedAll'] = model_user_item_info.predict(test_new_users.UserId, test_new_users.ItemId)\n",
    "test_new_items['PredictedAll'] = model_user_item_info.predict(test_new_items.UserId, test_new_items.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (user and item side info, warm start, extra users and items):  0.8902373699593187\n",
      "RMSE (user and item side info, cold start):  1.084223622310143\n",
      "RMSE (user and item side info, users trained without ratings, extra items):  0.9641877735092866\n",
      "RMSE (user and item side info, items trained without ratings, extra users):  1.0488749599247702\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE (user and item side info, warm start, extra users and items): \", np.sqrt(np.mean( (test_warm_start.PredictedAll - test_warm_start.Rating)**2) ))\n",
    "print(\"RMSE (user and item side info, cold start): \", np.sqrt(np.mean( (test_cold_start.PredictedAll - test_cold_start.Rating)**2) ))\n",
    "print(\"RMSE (user and item side info, users trained without ratings, extra items): \", np.sqrt(np.mean( (test_new_users.PredictedAll - test_new_users.Rating)**2) ))\n",
    "print(\"RMSE (user and item side info, items trained without ratings, extra users): \", np.sqrt(np.mean( (test_new_items.PredictedAll - test_new_items.Rating)**2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rho (user and item side info, warm start, extra users and items):  0.6167629307175454\n",
      "Rho (user and item side info, cold start):  0.32571107573887353\n",
      "Rho (user and item side info, users trained without ratings, extra items):  0.48530835816378215\n",
      "Rho (user and item side info, items trained without ratings, extra users):  0.3509155367711825\n"
     ]
    }
   ],
   "source": [
    "print(\"Rho (user and item side info, warm start, extra users and items): \", np.corrcoef(test_warm_start.PredictedAll, test_warm_start.Rating)[0][1])\n",
    "print(\"Rho (user and item side info, cold start): \", np.corrcoef(test_cold_start.PredictedAll, test_cold_start.Rating)[0][1])\n",
    "print(\"Rho (user and item side info, users trained without ratings, extra items): \", np.corrcoef(test_new_users.PredictedAll, test_new_users.Rating)[0][1])\n",
    "print(\"Rho (user and item side info, items trained without ratings, extra users): \", np.corrcoef(test_new_items.PredictedAll, test_new_items.Rating)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for top-10 recommendations (per user) from this model per configuration\n",
      "warm start, extra users and items:  3.986997084548105\n",
      "cold start:  3.9770367120081906\n",
      "users trained without ratings:  4.329006622516556\n",
      "items trained without ratings:  4.017231700471065\n"
     ]
    }
   ],
   "source": [
    "print('Average rating for top-10 recommendations (per user) from this model per configuration')\n",
    "print('warm start, extra users and items: ', test_warm_start.sort_values(['UserId','PredictedAll'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('cold start: ', test_cold_start.sort_values(['UserId','PredictedAll'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('users trained without ratings: ', test_new_users.sort_values(['UserId','PredictedAll'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('items trained without ratings: ', test_new_items.sort_values(['UserId','PredictedAll'], ascending=False).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user and item side information didn't seem to improve upon the model without that uses only ratings for warm-start recomendations, but it seems to perform very well for cold-start - almost as good as for warm start in fact.\n",
    "** *\n",
    "Alternative formulation with the \"offsets\" model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 0.940455\n",
      "  Number of iterations: 636\n",
      "  Number of functions evaluations: 694\n",
      "CPU times: user 35min 47s, sys: 2min 46s, total: 38min 33s\n",
      "Wall time: 8min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_user_item_info2 = CMF(k=50, reg_param=5e-3, offsets_model=True, random_seed=1)\n",
    "model_user_item_info2.fit(deepcopy(train),\n",
    "                          user_info=deepcopy(user_sideinfo_train),\n",
    "                          item_info=deepcopy(item_sideinfo_pca_train))\n",
    "test_warm_start['Predicted'] = model_user_item_info2.predict(test_warm_start.UserId, test_warm_start.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.64 s, sys: 76 ms, total: 3.72 s\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for u in list(np.unique(np.r_[test_new_users.UserId, test_cold_start.UserId])):\n",
    "    user_vec = deepcopy(user_side_info.loc[user_side_info.UserId == u])\n",
    "    del user_vec['UserId']\n",
    "    user_vec = user_vec.values.reshape((1, -1))\n",
    "    model_user_item_info2.add_user(new_id = u, attributes = user_vec)\n",
    "\n",
    "for i in list(np.unique(np.r_[test_new_items.ItemId.unique(), test_cold_start.ItemId.unique()])):\n",
    "    item_vec = deepcopy(item_sideinfo_pca.loc[item_sideinfo_pca.ItemId == i])\n",
    "    if item_vec.shape[0] > 0:\n",
    "        del item_vec['ItemId']\n",
    "        item_vec = item_vec.values.reshape((1, -1))\n",
    "        model_user_item_info2.add_item(new_id = i, attributes = item_vec)\n",
    "test_new_users['Predicted'] = model_user_item_info2.predict(test_new_users.UserId, test_new_users.ItemId)\n",
    "test_new_items['Predicted'] = model_user_item_info2.predict(test_new_items.UserId, test_new_items.ItemId)\n",
    "test_cold_start['Predicted'] = model_user_item_info2.predict(test_cold_start.UserId, test_cold_start.ItemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (user and item side info, warm start, extra users and items):  0.9278728563139468\n",
      "RMSE (user and item side info, cold start, users and items added afterwards):  0.985382884450961\n",
      "RMSE (user and item side info, users added afterwards):  0.9625519580300425\n",
      "RMSE (user and item side info, items added afterwards):  0.9547007567277902\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE (user and item side info, warm start, extra users and items): \", np.sqrt(np.mean( (test_warm_start.Predicted - test_warm_start.Rating)**2) ))\n",
    "print(\"RMSE (user and item side info, cold start, users and items added afterwards): \", np.sqrt(np.mean( (test_cold_start.Predicted - test_cold_start.Rating)**2) ))\n",
    "print(\"RMSE (user and item side info, users added afterwards): \", np.sqrt(np.mean( (test_new_users.Predicted - test_new_users.Rating)**2) ))\n",
    "print(\"RMSE (user and item side info, items added afterwards): \", np.sqrt(np.mean( (test_new_items.Predicted - test_new_items.Rating)**2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rho (user and item side info, warm start, extra users and items):  0.5778513879855706\n",
      "Rho (user and item side info, cold start, users and items added afterwards):  0.4375213405664898\n",
      "Rho (user and item side info, users added afterwards):  0.4834303546281004\n",
      "Rho (user and item side info, items added afterwards):  0.541600209425479\n"
     ]
    }
   ],
   "source": [
    "print(\"Rho (user and item side info, warm start, extra users and items): \", np.corrcoef(test_warm_start.Predicted, test_warm_start.Rating)[0][1])\n",
    "print(\"Rho (user and item side info, cold start, users and items added afterwards): \", np.corrcoef(test_cold_start.Predicted, test_cold_start.Rating)[0][1])\n",
    "print(\"Rho (user and item side info, users added afterwards): \", np.corrcoef(test_new_users.Predicted, test_new_users.Rating)[0][1])\n",
    "print(\"Rho (user and item side info, items added afterwards): \", np.corrcoef(test_new_items.Predicted, test_new_items.Rating)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for top-10 recommendations (per user) from this model per configuration\n",
      "warm start:  3.967142857142857\n",
      "cold start:  4.011847301448004\n",
      "users added afterwards:  4.313907284768212\n",
      "items added afterwards:  4.021283346757462\n"
     ]
    }
   ],
   "source": [
    "print('Average rating for top-10 recommendations (per user) from this model per configuration')\n",
    "print('warm start: ', test_warm_start.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('cold start: ', test_cold_start.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('users added afterwards: ', test_new_users.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())\n",
    "print('items added afterwards: ', test_new_items.sort_values(['UserId','Predicted'], ascending=False).groupby('UserId')['Rating'].head(10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seems again to perform better for cold-start recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p7\"></a>\n",
    "# 7. Examining some recomendations\n",
    "\n",
    "Now I'll examine the Top-10 recommended movies for some random users under different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# aggregate statistics\n",
    "avg_movie_rating = defaultdict(lambda: 0)\n",
    "num_ratings_per_movie = defaultdict(lambda: 0)\n",
    "for i in train.groupby('ItemId')['Rating'].mean().to_frame().itertuples():\n",
    "    avg_movie_rating[i.Index] = i.Rating\n",
    "for i in train.groupby('ItemId')['Rating'].agg(lambda x: len(tuple(x))).to_frame().itertuples():\n",
    "    num_ratings_per_movie[i.Index] = i.Rating\n",
    "\n",
    "# function to print recommended lists more nicely\n",
    "def print_reclist(reclist):\n",
    "    list_w_info = [str(m + 1) + \") - \" + movie_id_to_title[reclist[m]] +\\\n",
    "        \" - Average Rating: \" + str(np.round(avg_movie_rating[reclist[m]], 2))+\\\n",
    "        \" - Number of ratings: \" + str(num_ratings_per_movie[reclist[m]])\\\n",
    "                   for m in range(len(reclist))]\n",
    "    print(\"\\n\".join(list_w_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User with ID = 948 - this user was in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations from ratings-only model:\n",
      "1) - Raiders of the Lost Ark (1981) - Average Rating: 4.47 - Number of ratings: 1595\n",
      "2) - Rear Window (1954) - Average Rating: 4.46 - Number of ratings: 668\n",
      "3) - Wrong Trousers, The (1993) - Average Rating: 4.51 - Number of ratings: 558\n",
      "4) - Double Indemnity (1944) - Average Rating: 4.4 - Number of ratings: 360\n",
      "5) - Singin' in the Rain (1952) - Average Rating: 4.27 - Number of ratings: 482\n",
      "6) - Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) - Average Rating: 4.54 - Number of ratings: 404\n",
      "7) - M (1931) - Average Rating: 4.32 - Number of ratings: 196\n",
      "8) - Beauty and the Beast (1991) - Average Rating: 3.84 - Number of ratings: 658\n",
      "9) - Third Man, The (1949) - Average Rating: 4.46 - Number of ratings: 308\n",
      "10) - Schindler's List (1993) - Average Rating: 4.5 - Number of ratings: 1475\n",
      "------\n",
      "Recommendations from ratings + user demographics model:\n",
      "1) - Raiders of the Lost Ark (1981) - Average Rating: 4.47 - Number of ratings: 1595\n",
      "2) - Rear Window (1954) - Average Rating: 4.46 - Number of ratings: 668\n",
      "3) - Singin' in the Rain (1952) - Average Rating: 4.27 - Number of ratings: 482\n",
      "4) - Wrong Trousers, The (1993) - Average Rating: 4.51 - Number of ratings: 558\n",
      "5) - Beauty and the Beast (1991) - Average Rating: 3.84 - Number of ratings: 658\n",
      "6) - Double Indemnity (1944) - Average Rating: 4.4 - Number of ratings: 360\n",
      "7) - M (1931) - Average Rating: 4.32 - Number of ratings: 196\n",
      "8) - Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) - Average Rating: 4.54 - Number of ratings: 404\n",
      "9) - Invasion of the Body Snatchers (1956) - Average Rating: 3.89 - Number of ratings: 395\n",
      "10) - Third Man, The (1949) - Average Rating: 4.46 - Number of ratings: 308\n",
      "------\n",
      "Recommendations from ratings + movie tags model:\n",
      "1) - Schindler's List (1993) - Average Rating: 4.5 - Number of ratings: 1475\n",
      "2) - Raiders of the Lost Ark (1981) - Average Rating: 4.47 - Number of ratings: 1595\n",
      "3) - Beauty and the Beast (1991) - Average Rating: 3.84 - Number of ratings: 658\n",
      "4) - Wrong Trousers, The (1993) - Average Rating: 4.51 - Number of ratings: 558\n",
      "5) - Singin' in the Rain (1952) - Average Rating: 4.27 - Number of ratings: 482\n",
      "6) - Rear Window (1954) - Average Rating: 4.46 - Number of ratings: 668\n",
      "7) - Double Indemnity (1944) - Average Rating: 4.4 - Number of ratings: 360\n",
      "8) - Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) - Average Rating: 4.54 - Number of ratings: 404\n",
      "9) - M (1931) - Average Rating: 4.32 - Number of ratings: 196\n",
      "10) - Third Man, The (1949) - Average Rating: 4.46 - Number of ratings: 308\n",
      "------\n",
      "Recommendations from ratings + user demographics + movie tags model:\n",
      "1) - Boys Don't Cry (1999) - Average Rating: 3.98 - Number of ratings: 534\n",
      "2) - African Queen, The (1951) - Average Rating: 4.26 - Number of ratings: 663\n",
      "3) - Singin' in the Rain (1952) - Average Rating: 4.27 - Number of ratings: 482\n",
      "4) - Schindler's List (1993) - Average Rating: 4.5 - Number of ratings: 1475\n",
      "5) - Raiders of the Lost Ark (1981) - Average Rating: 4.47 - Number of ratings: 1595\n",
      "6) - Grand Illusion (Grande illusion, La) (1937) - Average Rating: 4.32 - Number of ratings: 113\n",
      "7) - M (1931) - Average Rating: 4.32 - Number of ratings: 196\n",
      "8) - Amadeus (1984) - Average Rating: 4.25 - Number of ratings: 884\n",
      "9) - Babe (1995) - Average Rating: 3.87 - Number of ratings: 1095\n",
      "10) - Shadow of a Doubt (1943) - Average Rating: 4.27 - Number of ratings: 145\n",
      "------\n",
      "Recommendations from ratings + user demographics + movie tags model (alternative formulation):\n",
      "1) - Best Years of Our Lives, The (1946) - Average Rating: 0 - Number of ratings: 0\n",
      "2) - City Lights (1931) - Average Rating: 0 - Number of ratings: 0\n",
      "3) - Children of Heaven, The (Bacheha-Ye Aseman) (1997) - Average Rating: 4.24 - Number of ratings: 42\n",
      "4) - Glory (1989) - Average Rating: 0 - Number of ratings: 0\n",
      "5) - Schindler's List (1993) - Average Rating: 4.5 - Number of ratings: 1475\n",
      "6) - Shawshank Redemption, The (1994) - Average Rating: 4.54 - Number of ratings: 1394\n",
      "7) - Paris Was a Woman (1995) - Average Rating: 2.0 - Number of ratings: 2\n",
      "8) - Great Escape, The (1963) - Average Rating: 0 - Number of ratings: 0\n",
      "9) - Central Station (Central do Brasil) (1998) - Average Rating: 0 - Number of ratings: 0\n",
      "10) - Modern Times (1936) - Average Rating: 0 - Number of ratings: 0\n"
     ]
    }
   ],
   "source": [
    "reclist1 = model_no_side_info.topN(user=948, n=10, exclude_seen=True)\n",
    "reclist2 = model_user_info_all.topN(user=948, n=10, exclude_seen=True)\n",
    "reclist3 = model_item_info_all.topN(user=948, n=10, exclude_seen=True)\n",
    "reclist4 = model_user_item_info.topN(user=948, n=10, exclude_seen=True)\n",
    "reclist5 = model_user_item_info2.topN(user=948, n=10, exclude_seen=True)\n",
    "\n",
    "print('Recommendations from ratings-only model:')\n",
    "print_reclist(reclist1)\n",
    "print(\"------\")\n",
    "print('Recommendations from ratings + user demographics model:')\n",
    "print_reclist(reclist2)\n",
    "print(\"------\")\n",
    "print('Recommendations from ratings + movie tags model:')\n",
    "print_reclist(reclist3)\n",
    "print(\"------\")\n",
    "print('Recommendations from ratings + user demographics + movie tags model:')\n",
    "print_reclist(reclist4)\n",
    "print(\"------\")\n",
    "print('Recommendations from ratings + user demographics + movie tags model (alternative formulation):')\n",
    "print_reclist(reclist5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User with ID = 1 - this user was not in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations from ratings + user demographics model:\n",
      "1) - Shawshank Redemption, The (1994) - Average Rating: 4.54 - Number of ratings: 1394\n",
      "2) - Schindler's List (1993) - Average Rating: 4.5 - Number of ratings: 1475\n",
      "3) - Usual Suspects, The (1995) - Average Rating: 4.53 - Number of ratings: 1116\n",
      "4) - Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) - Average Rating: 4.54 - Number of ratings: 404\n",
      "5) - Godfather, The (1972) - Average Rating: 4.54 - Number of ratings: 1414\n",
      "6) - Wrong Trousers, The (1993) - Average Rating: 4.51 - Number of ratings: 558\n",
      "7) - Close Shave, A (1995) - Average Rating: 4.5 - Number of ratings: 415\n",
      "8) - Raiders of the Lost Ark (1981) - Average Rating: 4.47 - Number of ratings: 1595\n",
      "9) - Rear Window (1954) - Average Rating: 4.46 - Number of ratings: 668\n",
      "10) - Sixth Sense, The (1999) - Average Rating: 4.4 - Number of ratings: 1560\n",
      "------\n",
      "Recommendations from ratings + user demographics + movie tags model:\n",
      "1) - Schindler's List (1993) - Average Rating: 4.5 - Number of ratings: 1475\n",
      "2) - Shawshank Redemption, The (1994) - Average Rating: 4.54 - Number of ratings: 1394\n",
      "3) - Life Is Beautiful (La Vita ï¿½ bella) (1997) - Average Rating: 4.33 - Number of ratings: 731\n",
      "4) - Close Shave, A (1995) - Average Rating: 4.5 - Number of ratings: 415\n",
      "5) - Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954) - Average Rating: 4.54 - Number of ratings: 404\n",
      "6) - Sanjuro (1962) - Average Rating: 4.69 - Number of ratings: 45\n",
      "7) - Wrong Trousers, The (1993) - Average Rating: 4.51 - Number of ratings: 558\n",
      "8) - Roman Holiday (1953) - Average Rating: 4.26 - Number of ratings: 260\n",
      "9) - Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) - Average Rating: 4.48 - Number of ratings: 291\n",
      "10) - Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963) - Average Rating: 4.44 - Number of ratings: 881\n",
      "------\n",
      "Recommendations from ratings + user demographics + movie tags model (alternative formulation):\n",
      "1) - Shawshank Redemption, The (1994) - Average Rating: 4.54 - Number of ratings: 1394\n",
      "2) - City Lights (1931) - Average Rating: 0 - Number of ratings: 0\n",
      "3) - Schindler's List (1993) - Average Rating: 4.5 - Number of ratings: 1475\n",
      "4) - Charade (1963) - Average Rating: 4.16 - Number of ratings: 177\n",
      "5) - Usual Suspects, The (1995) - Average Rating: 4.53 - Number of ratings: 1116\n",
      "6) - Central Station (Central do Brasil) (1998) - Average Rating: 0 - Number of ratings: 0\n",
      "7) - Modern Times (1936) - Average Rating: 0 - Number of ratings: 0\n",
      "8) - Children of Heaven, The (Bacheha-Ye Aseman) (1997) - Average Rating: 4.24 - Number of ratings: 42\n",
      "9) - Foreign Correspondent (1940) - Average Rating: 0 - Number of ratings: 0\n",
      "10) - Thin Man, The (1934) - Average Rating: 0 - Number of ratings: 0\n"
     ]
    }
   ],
   "source": [
    "# reclist1 = model_no_side_info.topN(user=1, n=10) # not possible with this model\n",
    "reclist2 = model_user_info_all.topN(user=1, n=10)\n",
    "# reclist3 = model_item_info_all.topN(user=1, n=10) # not possible with this model\n",
    "reclist4 = model_user_item_info.topN(user=1, n=10)\n",
    "reclist5 = model_user_item_info2.topN(user=1, n=10)\n",
    "\n",
    "# print('Recommendations from ratings-only model:')\n",
    "# print_reclist(reclist1)\n",
    "# print(\"------\")\n",
    "print('Recommendations from ratings + user demographics model:')\n",
    "print_reclist(reclist2)\n",
    "# print(\"------\")\n",
    "# print('Recommendations from ratings + movie tags model:')\n",
    "# print_reclist(reclist3)\n",
    "print(\"------\")\n",
    "print('Recommendations from ratings + user demographics + movie tags model:')\n",
    "print_reclist(reclist4)\n",
    "print(\"------\")\n",
    "print('Recommendations from ratings + user demographics + movie tags model (alternative formulation):')\n",
    "print_reclist(reclist5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from these lists, the alternative formulation of the model tends to recommend more movies that were not in the training set, which in many contexts would be a desirable thing despite the slightly lower achieved metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
